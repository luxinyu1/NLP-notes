# 信息论

## 熵

在信息论中，熵是宏观态对于观察者的**不确定性**。

比如模型要学习一个概率分布用于 $k$ 分类问题，宏观态就是该样本预测本身的答案，也就是一个 one-hot 的 label $y$，观察者就是一个完全未经训练的模型，该模型当前对于任何样本都有预测 $\hat{y} = \{\frac{1}{k}, \frac{1}{k},\ldots, \frac{1}{k}\}$，熵就是 $y$ 对于 $\hat{y}$ 的不确定性。

信息则是能够消除熵的事物，比如我们可以向模型提供信息，使得 $y$ 更接近于 $\hat{y}$。

因此熵又被称为自信息 (self-information) ，可以用于度量信息量的大小，其公式为：

$$
H(x) = -\sum_{x\in R}p(x)log_{2}p(x)
$$

当我们定义质量时，我们选定了一个物体所拥有的质量为 1kg，随后所有以 kg 为单位的质量都是**参照**该物体。

类似地，当我们定义信息时也需要选择一个参照，所有信息量的度量就可以基于这个参照进行描述。

这个参照我们可以选择为一个二项分布，此时熵的单位就是 bit。例如抛硬币，不确定性对于概率是一种以 2 为底的负对数关系，比如抛两枚硬币，不确定性是 2 ，而结果却有 4 种可能，对于每种可能，其概率是 1/4。用这个负对数的关系就可以定义信息量了：

$$
I(x) = -log_{2}p(x)
$$

取个极端情况理解一下，当 $p(x)=1$ 也就是该事件必然发生时，信息量为 0 。

同时，**信息熵是信息量的期望**，因为现实中一个系统并不会像是抛硬币这样的独立事件，将对应的微观不确定性乘上其发生的概率加权，最后求和，就可以得到整个系统的信息熵。

$$
H(x) = \mathbb{E}_{x \sim p} I(x)
$$

熵从霍夫曼编码的角度理解，也代表着当编码方案完美时，最短平均编码长度的是多少。

## 交叉熵

交叉熵用于衡量**当前**宏观态对于观察者的不确定性。

回到那个 $k$ 分类问题，现在模型已经学到一些信息了，它对预测问题有了一些自己的想法，交叉熵就是现在 $y$ 对于 $\hat{y}$ 的不确定性。

因此，对于数据集$D_c$，很自然的我们把熵改写成这样，以2为底的对数变为以 e 为底的自然对数，方便计算，得到交叉熵：

$$
\text{cross\_entropy}(x) = -\sum_{x\in D_c}y logp(c \mid x)
$$

从霍夫曼编码的角度理解，交叉熵是使用当前观察者观察到的分布对信息进行霍夫曼编码，这是平均编码长度一定不是最完美的，也就是**交叉熵一定大于熵**。

## 相对熵

相对熵又被称为 KL 散度，是交叉熵与熵的差。也就是当前分布与真实分布的信息损耗。即：

$$
D = -\sum_{x\in D_c}y \log p(c \mid x) +\sum_{x\in R} y\log y
$$

这里的损耗，也就是模型为了完美观察宏观态还需要输入的信息，当这个信息非常接近于 0 时，我们就会觉得模型对于宏观态观察的很充足了，拟合地就很好了。

当然，更一般化地，相对熵也可以独立地表示**两个概率分布的相对差距**，比如 $p(x)$ 与 $q(x)$ 的相对熵就可以定义为：

$$
D(p||q)=\sum_{x \in X}p(x) \log(\frac{p(x)}{q(x)})=\mathbb{E}_{x \sim p} \log\frac{p(x)}{q(x)}
$$

### 非负性

KL 散度一定是大于等于 0 的，并且当且仅当 $$p(x)=q(x)$$ 时等号成立，这点可以由 Jensen 不等式得到：

>  Jenson 不等式：当 $f$ 为凸函数，即 $f''(x) \ge 0 $ 时，有 $ \mathbb{E}(f(x)) \ge f(\mathbb{E}(x))$。

$$
\begin{aligned}
\mathrm{D}(p \| q) &=\sum_{x \in X} p(x) \left[\log \left(\frac{p(x)}{q(x)}\right)\right]=E_{p(x)}\left[\log \left(\frac{p(x)}{q(x)}\right)\right] \\
&=-E_{p(x)}\left[\log \left(\frac{q(x)}{p(x)}\right)\right] \\
& \geq-\log \left(\sum_{x \in X} p(x) \frac{q(x)}{p(x)}\right)=-\log \left(\sum_{x \in X} q(x)\right)=0
\end{aligned}
$$

## 参考

https://www.youtube.com/watch?v=NOQOow64r0w

https://www.zhihu.com/question/310100965

https://zhuanlan.zhihu.com/p/28010501

《统计自然语言处理》