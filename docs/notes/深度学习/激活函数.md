# 激活函数

激活函数是一种按元素 (elemental-wise) 的**非线性函数**。

在激活函数提出之前，神经网络不具有对非线性数据的表征能力，无论神经网络有多少层，输出都是输入的线性组合，例如普通感知机 (Perceptron)： $y^{<i>}=Wx+b \in \R^m$ 。而真实数据往往是非线性的。激活函数可以将原来线性的神经元输出转化为非线性，从而从根本上提高整个网络的对于数据的拟合能力。

## ReLU (Restricted Linear Unit)

ReLU（修正线性单元），是一种相对**简单**的激活函数。
$$
\text{ReLU}(x) = max(x,0)
$$

<div align="center">
    <img src="https://nlp-notes.oss-cn-beijing.aliyuncs.com/imgs/ReLU.png" width="30%"/>
</div>
ReLU 的梯度计算也因此变得非常直接，当该神经元输出为负时，直接用梯度置 0 屏蔽该神经元，其余情况梯度均为 1。

但计算简单也是需要付出一定代价的，ReLU 主要有三个缺点：

1. 整流后的分布不是以 0 为中心。
2. 一旦一个神经元输入为负，该神经元将永远不会被激活，这种现象被称为**神经元坏死**。
3. ReLU 不会对数据做幅度压缩，所以数据的幅度会随着模型层数的增加不断扩张。

### ReLU 的变体

<div align="center">
    <img src="https://nlp-notes.oss-cn-beijing.aliyuncs.com/imgs/ReLUfamily.jpg" width="80%"/>
</div>


#### Leaky ReLU

Leaky ReLU 中的 $\alpha$ 为常数，一般设置 0.01。这个函数通常比 ReLU 激活函数效果要好，但是效果不是很稳定，所以在实际中 Leaky ReLU 使用的并不多。

#### PReLU

相比 Leaky ReLU，RReLU (Random ReLU) 中将 $\alpha$ 作为一个可学习的参数。

#### PReLU

RReLU（随机纠正线性单元）也是Leaky ReLU的一个变体。在 RReLU 中，负值的斜率在训练中是随机的，在测试中固定。RReLU 的亮点在于，在训练环节中，$ a_{ji} $ 是从一个均匀的分布 $ U(I,u) $ 中随机抽取的数值。

## Softmax

Softmax 通常被用在多分类的**最后一层**，有些资料上不把它称为激活函数，这里就不再纠结这些问题了。

试想一下，当我们搭建一个 $n$ 分类器时，直接将神经网络的最后一层 $n$ 个神经元的值预测向量 $Z=[z_1,\ldots,z_n]$ 直接作为这 $n$ 类的概率输出会发生什么？

首先，这些值中可能有负数，也可能会大于 1，违背了概率的性质。

其次，这些值总和可能不为 1，违背了一个概率分布的性质。

顺着这两点想，对每个未归一化的预测求幂，这样可以确保输出非负。为了确保最终输出的总和为 1 ，我们再对每个求幂后的结果除以它们的总和，这样就得到了 Softmax 函数：
$$
\text{Softmax} (z_{i})= \frac{e^{z_i}}{\sum_{j}{e^{z_j}}}
$$

### 反向传播

假设未归一化的预测向量 $Z$ 。经过 Softmax 后，我们得到了归一化的预测值 $A=\left[\frac{e^{z_{1}}}{\Sigma_{j=1}^{n} e^{z_{j}}}, \frac{e^{z_{2}}}{\Sigma_{j=1}^{n} e^{z_{j}}}, \ldots, \frac{e^{z_{n}}}{\Sigma_{j=1}^{n} e^{z_{j}}}\right]$，这里 $A$ 和 $ Z $ 的形状都为 $ (1,n) $。

Softmax 的损失函数通常是**交叉熵** (Cross Entropy Loss)，对于单个输入样本来说，交叉熵的形式是这样的：$\mathcal{L}=-\Sigma_{i=1}^{n} y_{i} \ln \left(a_{i}\right)$，其中 $\boldsymbol{y}$ 是独热标签，即只有一个位置 $ y_j=1 $，其余位置均为 0。显然，损失是一个标量。

此时，$\mathcal{L}=-y_jln(a_j)$，在反向传播中，这个节点我们想要求其对预测向量的梯度，即$\frac{\partial \mathcal{L}}{\partial Z}$，对于一个标量对 $n$ 维行向量求导，最后的结果应该也是一个 $n$ 维行向量，即形状为$(1, n)$：

$$
\frac{\partial \mathcal{L}}{\partial Z}=\left[\frac{\partial \mathcal{L}}{\partial z_{1}}, \frac{\partial \mathcal{L}}{\partial z_{2}}, \ldots, \frac{\partial \mathcal{L}}{\partial z_{n}}\right]
$$
再由链式求导法则，可以得到：
$$
\frac{\partial \mathcal{L}}{\partial Z}=\frac{\partial \mathcal{L}}{\partial A} \frac{\partial A}{\partial Z}
$$
这里 $\frac{\partial A}{\partial Z}$ 是雅可比 (Jacobian) 式的矩阵微分，$ A $ 为行向量，$ Z $ 为行向量，最后得到的结果是 $(n,n)$ 的矩阵：
$$
\frac{\partial \boldsymbol{A}}{\partial \boldsymbol{Z}}=\left[\begin{array}{ccc}
\frac{\partial a_{1}}{\partial z_{1}} & \cdots & \frac{\partial a_{1}}{\partial z_{n}} \\
\vdots & \ddots & \vdots \\
\frac{\partial a_{n}}{\partial z_{1}} & \ldots & \frac{\partial a_{n}}{\partial z_{n}}
\end{array}\right]
$$
试着算几个我们就可以知道，矩阵的对角线元素与其他元素的值是两种不同的形式。对于第 $i$ 行的对角线元素：
$$
\frac{\partial a_{i}}{\partial z_{i}} = \frac{e^{z_{i}} \sum_{j=1}^{n} e^{z_{j}}-e^{z_{i}} e^{z_{i}}}{\left(\sum_{j=1}^{n} e^{z_{j}}\right)^{2}}=a_i-a_i^2
$$
对于位于第 $i$ 行第 $j$ 列的非对角线元素：
$$
\frac{\partial a_{i}}{\partial z_{j}} = \frac{0-e^{z_{j}} e^{z_{i}}}{\left(\sum_{j=1}^{n} e^{z_{j}}\right)^{2}}=-a_ja_i
$$


核对一下维度，$(1,n)=(1,n)\times(n,n)$ ，正确。

根据前面的分析我们可以知道，$ \frac{\partial \mathcal{L}}{\partial A} $ 只在 $j$ 位置非 0，值为 $-\frac{1}{a_{j}}$ 即：
$$
\frac{\partial \mathcal{L}}{\partial A}=\left[\frac{\partial \mathcal{L}}{\partial a_{1}}, \frac{\partial \mathcal{L}}{\partial a_{2}}, \ldots, \frac{\partial \mathcal{L}}{\partial a_{j}},\ldots,\frac{\partial \mathcal{L}}{\partial a_{n}}\right] = [0,0,\ldots,-\frac{1}{a_{j}},\ldots,0]
$$
那么两个偏导项相乘，也就是只取了矩阵第 $j$ 行的元素乘一个 $-\frac{1}{a_{j}}$ 的系数，对于 $i=j$ 位置，即
$$
\frac{\partial \mathcal{L}}{\partial Z}=\frac{\partial \mathcal{L}}{\partial A} \frac{\partial A}{\partial Z}=(a_j-a_j^2)(-\frac{1}{a_{j}})=a_j-1
$$

对于 $ i\ne j $ 位置，有：
$$
\frac{\partial \mathcal{L}}{\partial Z}=\frac{\partial \mathcal{L}}{\partial A} \frac{\partial A}{\partial Z}=(-a_ja_i)(-\frac{1}{a_{j}})=a_i
$$
即$\frac{\partial L}{\partial \boldsymbol{z}}=\left[a_{1}, a_{2}, \ldots, a_{j}-1, \ldots a_{n}\right]=\boldsymbol{a}-\boldsymbol{y}$，这个结果非常简单精巧，也是交叉熵损失设计的绝妙之处。也就是反向传播 softmax+交叉熵损失节点的时候，完全不需要进行复杂的计算，梯度值就是原预测值减去标签。这个意义也不难理解，预测值与标签偏离地越远，梯度就越大，对应的更新也就越快。

## Sigmoid

Sigmoid 又称"S"函数，因为其形如"S"曲线。

<div align="center">
    <img src="https://nlp-notes.oss-cn-beijing.aliyuncs.com/imgs/sigmoid.png" width="40%"/>
</div>

与 Softmax 相对应，Sigmoid 如果被放在最后一层，可以应对二分类问题时的输出整流，实际上不难证明，Sigmoid 只是 Softmax 在二分类时的一个特例：
$$
\text{Sigmoid}(z)=\frac{1}{1+e^{-z}}=\frac{e^{z}}{e^{z}+1}=1-\text{Sigmoid}(-z)
$$

如果将其视作普通的在隐藏层间的激活函数，也是可行的，但是据吴恩达老师在实战中总结的经验，tanh几乎在任何时候都优于 Sigmoid 😅

主要 Sigmoid 有这几点缺点：

1. 需要算幂函数，计算速度自然就慢。
2. 不是以 0 为中心，收敛也慢。
3. 导数 $\text{Sigmoid}(z)(1-\text {Sigmoid}(z))$ 最大也只有 0.25，梯度小，由于链式法则是以乘积的形式累积梯度，反向传播到靠前的层的时候更新值太小，这样的现象被成为**梯度弥散**。

### 反向传播

同样地，我们仍假定使用交叉熵损失函数，但是注意，当二分类时，只需要一个标签位置就可以区分两个类，我们不需要再用 one-hot 编码，如果 $y$ 表示第 1 类的概率，$(1-y)$ 就可以表示第二类的概率，对于预测值也是如此。此时交叉熵的形式就变为：$\mathcal{L}=-(y \log a+\left(1-y\right) \log \left(1-a\right))$。

类似地，我们需要求：
$$
\frac{\partial \mathcal{L}}{\partial Z}=\frac{\partial \mathcal{L}}{\partial A} \frac{\partial A}{\partial Z}
$$
可以求得：
$$
\frac{\partial \mathcal{L}}{\partial A}=-\frac{y}{a}-\frac{1-y}{a-1}=\frac{y-a}{a(a-1)}
$$

$$
\frac{\partial A}{\partial Z}=\frac{e^{-z}}{(1+e^{-z})^2}=\text{Sigmoid}(z)(1-\text {Sigmoid}(z))=a(1-a)
$$

相乘我们就可以得到最终的导数：

$$
\frac{\partial \mathcal{L}}{\partial Z}=\frac{y-a}{a(a-1)}a(1-a)=a-y=\text{Sigmoid}(z)-y
$$

**神奇的事发生了，我们发现 Sigmoid 的导数形式也和 Softmax 一致，都是预测值减去标签！**

## tanh

tanh 为双曲正切函数，也可以理解为双曲正弦函数与双曲余弦函数的商。
$$
\tanh (x)=\frac{\sinh (x)}{\cosh (x)}=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}
$$

<div align="center">
    <img src="https://nlp-notes.oss-cn-beijing.aliyuncs.com/imgs/tanh.png" width="40%"/>
</div>

对其求导：
$$
\tanh' (x) = \frac{(e^x+e^{-x})^2-(e^{x}-e^{-x})^2}{(e^x+e^{-x})^2}= 1-\tanh^2(x)
$$
由于 tanh 是以 0 为中心的，可以从正负两个方向加速收敛，同时，导数的相对数值也更大。因此在这一点上它的性能是优于 Sigmoid 的。

### 与 Sigmoid 的关系

从图像上看起来，tanh 像是被拉长平移后的 Sigmoid，从数学上也可以证明，tanh 是 Sigmoid 的一个拉伸变换：
$$
\tanh (x) = \frac{1-e^{-2x}}{1+e^{-2x}}=1-\frac{2e^{-2x}}{1+e^{-2x}}=1-2 \text{sigmoid (-2x)}
$$

## 参考

李沐 - 实用机器学习中文版视频

https://zh-v2.d2l.ai/chapter_linear-networks/softmax-regression.html

https://zhuanlan.zhihu.com/p/25723112

https://zhuanlan.zhihu.com/p/105758059

https://en.wikipedia.org/wiki/Sigmoid_function

